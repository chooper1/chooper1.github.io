{"basics":{"name":"Coleman Hooper","label":"PhD Student in AI Systems","image":"","email":"chooper@berkeley.edu","phone":"","summary":"PhD Student at UC Berkeley working on efficient LLM inference and AI systems optimization. Research focused on efficient long context inference, model compression, and hardware-software co-design.","location":{"address":"","postalCode":"94720","city":"Berkeley","countryCode":"US","region":"California"},"profiles":[{"network":"LinkedIn","username":"coleman-hooper-165061193","url":"https://www.linkedin.com/in/coleman-hooper-165061193/"},{"network":"GitHub","username":"chooper1","url":"https://github.com/chooper1"},{"network":"Google Scholar","username":"si-368wAAAAJ","url":"https://scholar.google.com/citations?user=si-368wAAAAJ&hl=en"}]},"work":[{"name":"NVIDIA","position":"Research Intern","url":"https://www.nvidia.com","startDate":"2024-06-01","endDate":"2024-08-31","summary":"Worked on hardware-software co-design for efficient LLM inference","highlights":["Hardware-software co-design","LLM inference optimization"]},{"name":"NVIDIA","position":"Research Intern","url":"https://www.nvidia.com","startDate":"2022-06-01","endDate":"2022-08-31","summary":"Developed algorithms for speech recognition and speaker diarization","highlights":["Speech processing","Speaker diarization","Algorithm development"]}],"volunteer":[],"education":[{"institution":"University of California, Berkeley","location":"Berkeley, CA, USA","url":"https://www.berkeley.edu/","area":"AI Systems","studyType":"PhD","startDate":"2022-08-01","endDate":"","score":"","courses":["Efficient LLM Inference","AI Systems","Computer Architecture"]},{"institution":"Harvard University","location":"Cambridge, MA, USA","url":"https://www.harvard.edu/","area":"Electrical Engineering","studyType":"Bachelor of Science (Summa Cum Laude)","startDate":"2019-09-01","endDate":"2022-05-31","score":"","courses":["Computer Systems","AI Hardware","Signal Processing"]},{"institution":"Acadia University","location":"Wolfville, NS, Canada","url":"https://www.acadiau.ca/","area":"Mathematics and Computer Science","studyType":"Bachelor of Science","startDate":"2017-09-01","endDate":"2019-05-31","score":"","courses":["Transferred after two years"]}],"awards":[{"title":"Berkeley EECS Department Graduate Fellowship","date":"2022-08-01","awarder":"UC Berkeley","url":"","summary":"Graduate fellowship for PhD studies in AI Systems"},{"title":"Sophia Freund Prize","date":"2022-05-31","awarder":"Harvard University","url":"","summary":"Awarded to Harvard seniors graduating summa cum laude with the highest GPA"}],"certificates":[],"publications":[{"name":"Multipole Attention for Efficient Long Context Reasoning","publisher":"NeurIPS","releaseDate":"2025-12-01","url":"https://arxiv.org/abs/2506.13059","summary":"We propose Multipole Attention, a novel attention mechanism that enables efficient processing of very long sequences by leveraging hierarchical representations and sparse attention patterns."},{"name":"Squeezed Attention: Accelerating Long Context Length LLM Inference","publisher":"ACL","releaseDate":"2025-07-01","url":"https://arxiv.org/abs/2411.09688","summary":"Squeezed Attention reduces the computational complexity of attention mechanisms for long context lengths, enabling efficient processing of very long sequences."},{"name":"KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization","publisher":"NeurIPS","releaseDate":"2024-12-01","url":"https://arxiv.org/abs/2401.18079","summary":"KVQuant enables efficient LLM inference with context lengths up to 10 million tokens through innovative KV cache quantization techniques."},{"name":"SqueezeLLM: Dense-and-Sparse Quantization","publisher":"ICML","releaseDate":"2024-07-01","url":"https://arxiv.org/abs/2306.07629","summary":"SqueezeLLM introduces dense-and-sparse quantization for efficient LLM inference, achieving significant compression with minimal accuracy loss."}],"skills":[{"name":"Machine Learning","level":"Expert","icon":"fa-solid fa-brain","keywords":["Large Language Models","Attention Mechanisms","Model Quantization","Efficient Inference"]},{"name":"Computer Systems","level":"Expert","icon":"fa-solid fa-microchip","keywords":["Hardware-Software Co-design","Computer Architecture","Memory Optimization","AI Accelerators"]},{"name":"Programming","level":"Expert","icon":"fa-solid fa-code","keywords":["Python","C++","CUDA","PyTorch","JAX"]}],"languages":[{"language":"English","fluency":"Native speaker","icon":""}],"interests":[{"name":"AI Systems","icon":"fa-solid fa-robot","keywords":["Efficient LLM Inference","Attention Mechanisms","Model Compression","Hardware-Software Co-design"]},{"name":"Computer Architecture","icon":"fa-solid fa-microchip","keywords":["AI Accelerators","Memory Optimization","Power-Efficient Computing","Edge AI"]}],"references":[{"name":"Professor Kurt Keutzer","icon":"fa-solid fa-user","reference":"Professor, Berkeley AI Research (BAIR), UC Berkeley"},{"name":"Professor Sophia Shao","icon":"fa-solid fa-user","reference":"Professor, SLICE Lab, UC Berkeley"}],"projects":[{"name":"KVQuant","summary":"Efficient LLM inference with KV cache quantization for context lengths up to 10 million tokens.","highlights":["NeurIPS 2024","Open Source","Long Context Processing"],"startDate":"2024-01-01","endDate":"","url":"https://github.com/SqueezeAILab/KVQuant"},{"name":"SqueezeLLM","summary":"Dense-and-sparse quantization for efficient LLM inference with minimal accuracy loss.","highlights":["ICML 2024","Model Compression","Open Source"],"startDate":"2023-01-01","endDate":"","url":"https://github.com/SqueezeAILab/SqueezeLLM"},{"name":"SqueezedAttention","summary":"Accelerating long context length LLM inference through efficient attention mechanisms.","highlights":["ACL 2025","Attention Optimization","Long Context"],"startDate":"2024-01-01","endDate":"","url":"https://github.com/SqueezeAILab/SqueezedAttention"}]}